{"cells":[{"cell_type":"markdown","id":"67e1bb61","metadata":{"id":"67e1bb61"},"source":["First we will import the regular Python libraries that we have been using.\n","\n","Normally you would import everything at the top here, but for demonstration purposes we will import functions as we need them."]},{"cell_type":"code","execution_count":null,"id":"5bb700b3","metadata":{"id":"5bb700b3"},"outputs":[],"source":["# IMPORTS\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# # libraries and functions used throughout:\n","\n","# from sklearn.model_selection import train_test_split\n","\n","# from sklearn.pipeline import Pipeline\n","# from sklearn.impute import SimpleImputer\n","# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","# from sklearn.compose import ColumnTransformer\n","\n","# from sklearn.ensemble import RandomForestRegressor\n","\n","# from sklearn.metrics import mean_squared_error, r2_score\n","\n","# from sklearn.model_selection import GridSearchCV"]},{"cell_type":"markdown","id":"b2994be3","metadata":{"id":"b2994be3"},"source":["Now, we will load the data.\n","\n","This dataset is from: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data\n","\n","The way this dataset was set up was with the training data in one file which includes the target variable \"SalePrice\" (the depdendent variable that we want to predict). So we load them as seperate dataframes."]},{"cell_type":"code","execution_count":null,"id":"b423f394","metadata":{"id":"b423f394"},"outputs":[],"source":["# LOAD DATA\n","\n","folder = 'house-prices-advanced-regression-techniques/'\n","fn_train = 'train.csv'\n","fn_test = 'test.csv'"]},{"cell_type":"markdown","id":"1f658329","metadata":{"id":"1f658329"},"source":["With the data loaded, we now want to do some exploratory data analysis, also called EDA.\n","\n","First we use the dataframe.describe() function to get a general overview of the statistics on each feature."]},{"cell_type":"code","execution_count":null,"id":"bff84604","metadata":{"scrolled":false,"id":"bff84604"},"outputs":[],"source":["# use df.describe() and df.info()"]},{"cell_type":"markdown","id":"878fd8fb","metadata":{"id":"878fd8fb"},"source":["Next what we want to do is take a closer look at the target and also some of the features we think\n","are the most interesting. What features do we think would be most important towards predicting the sale price?\n","\n","One thing we could do is do a pair plot of all the features. I won't do that right now because when I tried\n","it earlier it was taking too long to run, so for this demonstration i will plot the histogram, so the distribution,\n","of some of the features of interest."]},{"cell_type":"code","execution_count":null,"id":"8ac1c729","metadata":{"id":"8ac1c729"},"outputs":[],"source":["# # here is how we would plot the pairplot. notice how we take just the numerical features, not the categorical ones\n","# numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n","\n","# # create the pairplot using sns.pairplot with the subset of numerical columns\n","# import seaborn as sns\n","# sns.pairplot(df[numerical_cols])\n","\n","# general rule of thumb for number of bins in the square root of the number of datapoints\n","\n","# plot histograms here"]},{"cell_type":"markdown","id":"86bef73d","metadata":{"id":"86bef73d"},"source":["Now let's get an idea of how many NaN values we have in the dataframe. We will need to deal with these.\n","\n","Here are some ideas to keep in mind when dealing with NaN values in your data:\n","\n","- What percentage of the data are NaNs? If it's like 4% we can just drop them. If it's like 20% need to do   something. For example we could fill with a zero but need to think about if this makes sense. Could also take the average of the surrounding points.\n","\n","- This act of filling in NaNs is called **imputation** and we will discuss this.\n","\n","- Always need to fill or drop nans. Cannot send them to the model!! It's like putting $\\infty$ into a math equation.\n","\n","- Some models might warn you if there are nans.\n","\n","- How many **ROWS** contain NaNs is a good question to ask."]},{"cell_type":"code","execution_count":null,"id":"9f4fbed3","metadata":{"scrolled":false,"id":"9f4fbed3"},"outputs":[],"source":["# print each feature along with its number of NaN values\n","\n","# calculate nan statistics here"]},{"cell_type":"markdown","id":"314b9c66","metadata":{"id":"314b9c66"},"source":["We can see that there are NaN values. We will first define $X$ and $y$ along with splittng the data, and then do the preprocessing.\n","\n","Preprocessing involes:\n","\n","- imputing the data (filling in the NaN values)\n","- scaling the numerical data so that all of the features are in the same range\n","- encoding the categorical variables\n","\n","This step is splitting the data from 'train.csv' into training and validation subsets.\n","\n","The training data will be used to train the model. The validation data will be used to estimate the performance of the model we will set up. In the case of this data, we have the \"known target values\" for the training (and therefore the validation) data, but the test data has no \"known value\" (you can check 'test.csv', there is no 'SalePrice' column.\n","\n","We will use the 80/20 split."]},{"cell_type":"code","execution_count":null,"id":"3803aa30","metadata":{"id":"3803aa30"},"outputs":[],"source":["# split for training and testing\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","id":"5a0779db","metadata":{"id":"5a0779db"},"source":["Now with $X$ and $y$ defined we can carry out the preprocessing.\n","\n","Sources:\n","\n","SimpleImputer: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n","\n","StandardScalar: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n","\n","OneHotEncoder: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n","\n","One hot encoding further explained: https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n","\n","First we get the numeric features as a list, and the categorical features as list.\n","\n","Next we use the \"numeric_transformer\" to set how we want to deal with missing values, as well as scaling the features to be similar. The \"categorical_transformer\" does the same thing with the categorical variables which are described by strings as opposed to numbers.\n","\n","Last we define the preprocessor, where we apply the numerical transformer to the numerical data and we apply the categorical transformer to the categorical data.\n","\n","This pre processing will then be combined with the regression model later."]},{"cell_type":"code","execution_count":null,"id":"cad35063","metadata":{"id":"cad35063"},"outputs":[],"source":["# PREPROCESSING STEPS FOR NUMERICAL AND CATEGORICAL FEATURES\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","# this gets the numeric features as a list, these will either be an integer or a float\n","# numeric_features =\n","\n","# similarly this gets the categorical features as a list. these are strings and not numbers, for example could be 'yes' or 'no'\n","# categorical_features =\n","\n","# this defines how we want to transform the numerical features\n","# numeric_transformer =\n","\n","\n","# this defines how we want to transform the categorical features\n","# categorical_transformer =\n","\n","\n","# this now applies the column transforms we just defined\n","# preprocessor ="]},{"cell_type":"markdown","id":"36bbbd5d","metadata":{"id":"36bbbd5d"},"source":["We are now ready to define our regression model\n","\n","For this analysis we will be using the scikit-lean RandomForestRegressor. This comes from the sklearn.ensemble library which is a set of **ensemble** methods, meaning methods that involve multiple sub-methods. In this case, the data is put through a random forest before the regression is calculated.\n","\n","A decision tree in ML is essentially a set of if and else statements to subset data. A decision tree in ML is essentially a set of if and else statements to subset data. A random forest is a collection of decision trees where the if else statements are slightly different to get a more robust estimate.\n","\n","It is useful here to consider because there are many features to consider which affect the final sale price.\n","\n","The hyperparameter in the sklearn RandomForestRegressor is called n_estimators which is the number of decision trees. A higher number will vary more of the parameters and can give you a more accurate result, but will take longer to compute. Also you want to be careful of overfitting.\n","\n","RandomForestRegressor docs: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"]},{"cell_type":"code","execution_count":null,"id":"de7fb952","metadata":{"id":"de7fb952"},"outputs":[],"source":["# define random forest regressor model\n","from sklearn.ensemble import RandomForestRegressor\n","\n","number_of_trees = 100 # set the number of trees in the forest\n","# model ="]},{"cell_type":"markdown","id":"484d8aad","metadata":{"id":"484d8aad"},"source":["With the Random Forest Regression model defined, we now combine it with the preprocessing that we defined.\n","\n","Together this forms the 'full_model'. This model will then be trained (also known as 'fit'), and then used to make predictions."]},{"cell_type":"code","execution_count":null,"id":"f774946c","metadata":{"id":"f774946c"},"outputs":[],"source":["# define full model\n","\n","# full_model ="]},{"cell_type":"markdown","id":"57e9cf12","metadata":{"id":"57e9cf12"},"source":["With the full model defined, it can now be fit using the **TRAINING DATA**.\n","\n","This is where the model attempts to learn the underlying patterns.\n","\n","Since there are many dimensions this is why it is useful to use a regression model that first uses a random forest to subset the data."]},{"cell_type":"code","execution_count":null,"id":"4467edb1","metadata":{"id":"4467edb1"},"outputs":[],"source":["# fit model to data\n","\n","'''\n","do not need to convert the dataframes to np arrays anymore, FOR THE MOST PART. this is an older practice\n","check the docs for which ever algorithm you are using\n","'''\n","\n","\n","None # I just put this here to suppress the output of the fit. if you want you can delete this line and see what happens"]},{"cell_type":"markdown","id":"4e91a368","metadata":{"id":"4e91a368"},"source":["With our model defined and trained, we can now make predictions on the **VALIDATION DATA**.\n","\n","The terminology might be slightly confusing compared to the ML2 session where we made predictions on the test data.\n","\n","Terminology in ML can be loose in general. The concepts are more important than the words.\n","\n","In this specific case, we have the known values of the validation data. Meaning we have the house price of these rows.\n","\n","We will \"predict\" the sale price, and then compare these predicted values to the known values in order to get an estimation of the model's performance.\n","\n","We can evaluate the model performance by calculating the root mean squared error (RMSE) and R square ($R^2$).\n","\n","RMSE quantifies the average difference between the predicted values by your model and the actual values in your data. The lower the RMSE, the better the model fits the data, indicating a smaller average difference between predictions and actual values.\n","\n","R square is a statistical measure that represents the proportion of variance (spread) in the target variable that your model explains. A higher R square value (closer to 1) indicates a better fit, meaning your model explains a larger proportion of the variance in the target variable. An R square of 0 means the model explains none of the variance, essentially the same as predicting the average value for all data points.\n"]},{"cell_type":"code","execution_count":null,"id":"f6c8112b","metadata":{"scrolled":true,"id":"f6c8112b"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error, r2_score\n","\n","# make predictions on VALIDATION DATA\n","# y_val_pred =\n","\n","# evaluate model performance\n","\n","# calculate mean squared error, this tells us the average squared difference between the predicted and true values. so let's take the square root\n","# val_rmse =\n","\n","# r^2 tells us how well the model explains the variance in the data. an r^2 of 1 is a perfect correlation and a r^2 of  0 is no correlation\n","# val_r2 =\n","\n","# print out the metrics we just calculated\n","print(f\"Validation RMSE: {val_rmse}\") # this number is very large so we print the log10 of it\n","print(f\"Validation R2 Score: {val_r2}\")"]},{"cell_type":"markdown","id":"54a09833","metadata":{"id":"54a09833"},"source":["The $R^2$ we calculated looks good but the RMSE seems high. Let's compare it to the mean of the training target to get an idea of the RMSE as a percentage of the mean value."]},{"cell_type":"code","execution_count":null,"id":"60eab3ea","metadata":{"id":"60eab3ea"},"outputs":[],"source":["# print ratio of RMSE to mean training sale price\n","\n","# so the error is like 15%"]},{"cell_type":"markdown","id":"7c10c0ba","metadata":{"id":"7c10c0ba"},"source":["We now have an idea of the model's performance from the root mean squared error and $R^2$ (R squared) metrics.\n","\n","Let's now make a scatter plot comparing the \"predicted\" values to the actual values.\n","\n","Keep in mind this plot is telling us how well we made predictions on the **TRAINING DATA**. So it should follow very close to a straight line with slope = 1."]},{"cell_type":"code","execution_count":null,"id":"6ece4e90","metadata":{"scrolled":false,"id":"6ece4e90"},"outputs":[],"source":["# plot actual versus predicted for training set\n","\n","# this is how well the model fits the training set it was trained on\n","\n","# make prediction on training price data and plot it against the actual price data"]},{"cell_type":"markdown","id":"3bfb6e0f","metadata":{"id":"3bfb6e0f"},"source":["Now let's see how well the model generalizes to the \"unknown\" data which is in the **VALIDATION SET**.\n","\n","In this specific instance what I mean by unknown is that the model did not see this data during the training step."]},{"cell_type":"code","execution_count":null,"id":"83404653","metadata":{"scrolled":false,"id":"83404653"},"outputs":[],"source":["# plot actual vs predicted for validation set\n","\n","# this shows how the model performs on data it wasn't trained on. gives idea of how well it generalizes to new data\n","\n","plt.figure(dpi = 100)\n","plt.scatter(y_val, y_val_pred, alpha=0.5)\n","plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], '--', color='red')\n","plt.title(\"Actual vs Predicted (Validation)\")\n","plt.xlabel(\"Actual SalePrice\")\n","plt.ylabel(\"Predicted SalePrice\")\n","plt.show()"]},{"cell_type":"markdown","id":"c48d69c2","metadata":{"id":"c48d69c2"},"source":["The model is working ok, but could it be improved?\n","\n","Let's use hyperparameter tuning to find the best number of trees to use in the forest. This will effect how the data gets put into subsets before calculating the regression."]},{"cell_type":"code","execution_count":null,"id":"3a597b0e","metadata":{"id":"3a597b0e"},"outputs":[],"source":["# HYPERPARAMETER TUNING\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","# define hyperparameters and their range\n","# param_grid =\n","\n","# create gridsearchcv object\n","# grid_search =\n","\n","\n","# now use the grid search to fit the training data to find the optimal hyperparameters (in this case just one)\n","\n","# get the best parameters (the n_estimators which resulted in the best r^2 value)\n","# best_params =\n","\n","# print the best parameter\n","print(best_params)\n","\n","# {'regressor__n_estimators': 150}"]},{"cell_type":"markdown","id":"1cd686ea","metadata":{"id":"1cd686ea"},"source":["With our optimized value for n_estimators we can now re fit the model and see if the perfomance shifts."]},{"cell_type":"code","execution_count":null,"id":"752a9068","metadata":{"id":"752a9068"},"outputs":[],"source":["# define random forest regressor model\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# number_of_trees =\n","# model_optimized =\n","# define full optimized model\n","# full_model_optimized =\n","\n","# fit model to data\n","# full_model_optimized.\n","\n","# make predictions on validation data with optimized model\n","# y_val_pred =\n","\n","# calculate metrics on optimized model\n","val_mse = mean_squared_error(y_val, y_val_pred)\n","val_r2 = r2_score(y_val, y_val_pred)\n","print(f\"log Validation MSE: {np.log10(val_mse)}\")\n","print(f\"Validation R2 Score: {val_r2}\")"]},{"cell_type":"markdown","id":"c03b3570","metadata":{"id":"c03b3570"},"source":["Our metrics don't change too much... let's plot the new predictions on the validation data and examine."]},{"cell_type":"code","execution_count":null,"id":"a9e25563","metadata":{"scrolled":false,"id":"a9e25563"},"outputs":[],"source":["plt.figure(dpi = 100)\n","plt.scatter(y_val, y_val_pred, alpha=0.5)\n","plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], '--', color='red')\n","plt.title(\"Actual vs Predicted (Validation)\")\n","plt.xlabel(\"Actual SalePrice\")\n","plt.ylabel(\"Predicted SalePrice\")\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}